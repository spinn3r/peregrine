
- Extract jobs should probably be written to disk FIRST (or buffered in memory)
  so that I can compute the checksums.

- Add checksums to chunk files:

    http://www.cryptopp.com/benchmarks.html

    http://en.wikipedia.org/wiki/ZFS

    ZFS supports Fletcher and SHA2:

        http://en.wikipedia.org/wiki/Fletcher%27s_checksum
        http://en.wikipedia.org/wiki/SHA-2


    http://www.strchr.com/hash_functions

    Adler32 vs CRC32 vs Fletcher

    ... 
    

- Make all IO async and have a dedicated reader that reads disk chunks INTO
  memory.  Then I need a consumer which listens to the queue and performs work.
  This way IO is decoupled from CPU and I won't have to wait for disk to do CPU
  work.  This should be about a 10% performance gain.

- I think that all APIs should be cluster aware by default and that the Local
  version writes to the local disk and isn't meant to be used eternally.

  For example:

        PartitionWriter ... writes to all nr_replica machines that host that
        partition.

        LocalPartitionWriter does the writing on each partition on the other
        side of the network.

  For debug and engineering purposes (and testing) using LocalPartitionWriters
  should be easy since this would also make it easy to test.


- If I were to take an ENTIRE chunk, and read it into memory as 1 100MB byte
  array.  Then I could in theory keep a map of the position of a key, and the
  offset into the byte array.

  This way a read of a key would be:

    for( int i = chunk.offsets[0]; i < 8; ++i ) {
    }

  where offsets would contain the position of the key

  This would allow more efficient reading of keys as I could avoid having to do
  an arraycopy for each one.

  I should benchmark this though because the performance boost MIGHT not be
  worth the complexity.

    



- The mapper API should probably be like:


    Controller.map( InputReference() , mapper );
    Controller.reducer( reducer, OutputReference() );


    - At 25 parallel readers I could read at about 80MB/s or so... this means
      40MB/s write throughput to disk (or 40MB/s total input read throughput)
      ... so for 500GB this would mean. 208 minutes or 3.4 hours JUST for the
      reduce phase ... hm.... should I replicate this data to the other machines
      hosting that partition so that I can just quickly resume?  This is a tough
      decision.  I can see why compressing the output of mappers is important so
      that you can make the sort go faster by wrting more data... I have to see
      how CPU bound our reduce phase is going to be.


- Range routing has the advantage that I can keep things globally sorted AND
  group sorted... I just have to emit a different key based on position (or line
  number) in a document if I'm grepping.


- Cluster membership should be defined via gossip protocol in the future but for
  now the controller keeps track of everything.  

    - clusters should get a cluster identifier so that we can bring up multiple
      versions

- Heartbeat protocol should be used to verify that the machines are actually
  there.
   
   


- The architecture should be easy to understand and comprehend.  This means that
  optimizations in the framework can be easily implemented for new and external
  developers.





- On a 1TB disk, how fast do we want it to be available again over gigabit
  ethernet...
    

- step1...

    - benchmark the performance of taking a graph adjacency list / sparse matrix
      in the form of source, targets... and then invert it into target:
      sources... so that I can count the number of sources.

        the first map job should emit:
            (T1, S1)
            (T2, S1)
            (T3, S1)
            (T1, S2)

      And then I should group them like:

        (T1, (S1,S2))
        (T2, (S1))
        (T3, (S1))

      Then the reduce function should return:

        T1, 2
        T2, 1
        T3, 1


- partition setup... start with the number of machines.  The algorithm for
  determining the number of partitions is:

    nr_partitions = nr_hosts


- Using LARGER numbers of partitions means that when a machine fails that I can
1  restore to TWO machines and NOT just one...   TWO machines as the source and
  TWO as the target.  CLEARLY there is a sweet spot.


    - 1000 paritions + 100MB buffer == 100k per chunk written to disk.

    - potentially store all files to disk. how long would it take to read 100k
      at 100MB/s 1ms ... so this is 50% or the original performance... 



        

- One reason that the Pregel API is really cool is that you could implement the
  SAME framework on a single machine instance without a massive amount of wasted
  CPU time due to map reduce.  It COULD even be parallelized somewhat... 
        
- If the config is :

    - partition0: host0, host1, host2
    - partition1: host0, host1, host2

- The route() partition logic should be done via dependency injection so that we
  can make it faster by not having to do an isHashcode comparison for each
  route() usage.  This is a tight loop
    

- how do we store input text so that cat() works efficiently... with a custom
  partition handler.  Make it so tat lines 1-1000 show up on partition 0,
  1001-2000 on parittion 2, etc.
    

- AHHH!!! I put THREE partitions on EACH node... then I can run one reduce on
  EACH for each partition.  Since these boxes replicate these blocks, I can run
  ONE reduce per partition across all three hosts for VERY even CPU
  utilization.  If one of the boxes fails, the controller runs the job on one of
  the backup hosts.
    


- I need to think more about how I want to handle the shuffle stage... I think
  the Mapper should keep a handle on communications channels to shards and that
  ShuffleManager should JUST keep track of chunk job output ... this way if
  they're sorted it can just do a merge joing of the output on disk.

        
        
- we should fallocate chunks , especially chunks that are being written to by
  reducers, so they are on disk contibuous.
        

- I could compute my own minimum spanning tree so that I can route chunks from
  the controller directly and have machines distribute the data FOR me... The
  ideal design will be complicated and involve reading a few more research
  papers on the subject.
  
- compute hashcodes when reading and writing files... then compare the hashcode
  at the end to verify that I have the correct output.

- We have distinct phases that need to be documented: 
      
    - ExtractMap

    - ReduceMap, MergeMap

    - ReduceMerge (when one of the merge inputs comes from the reduce phase)
        - pagerank would require this...

    - ReduceMergeMap ... 


    - ReduceLoad
    - MergeLoad
    - instead of storing the output locally, just load it back into the remote
      storage system.

- The reduce phase should basically be a tiled mere join...

    - each chunk source in the reduce phase is a tile.

    - if the chunk does not have entries that are sorted, first sort each one, 

    - if we do NOT need to

    - the map output might in theory MUCH larger than the map input.
    
        
- Allow the programmer to work with bytes directly for performance reasons.
        
- Make the system so that the Extract phase can actually be the begging of the
  first map/reduce phase. Instead of FIRST writing to the DFS and THEN importing
  the data, if it is already ready to be mapped just read it and stream into the
  map system.

  
        
- The system is modeled after ETL jobs in the form of Extract, Transform, Load:

    http://en.wikipedia.org/wiki/Extract,_transform,_load

    Data is first written into maprunner by using an ExtractWriter and sending
    data to a given path in the system.  Input data is in the form of (K,V) and
    routed to the correct shard based on hash(K).

- The system was designed to be INSANELY small and easy to understand.  The
  original Map Reduce paper makes a great argument that all applications using
  the framework willr realize any performance optimization in the core.

  However, if your core is 100-200k lines of code, this is difficult to
  optimize.
  
- Data should naturally partition without much skew based on the same hash
  function between iterations.

   - Data based on fetching URLs would be a good example.

- Optimized for short running ETL jobs.  Data is designed to be stored within
  exported within maprunner on the local DFS points but isn't meant to be used
  to serve operational reads.  At the end of your map jobs you should export the
  data to your cluster in a system like Cassandra, MySQL, HBase, etc.

- The major optimization is for tasks that join against previously executed map
  reduce jobs.  This optimization is present because we shard the data and store
  the data in a given key on the same machine.  This way it's possible to do a
  map-side join of the data reading from the local host.

 - 
