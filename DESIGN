







- range routing has the advantage that I can keep things globally sorted AND
  group sorted... I just have to emit a different key based on position.



- cluster membership should be defined via gossip protocol
    - maybe with confirmation
- clusters should get a cluster identifier so that we can bring up multiple
  versions

- heartbeat protocol should be used to verify that the machines are actually
  there.
   
   







- using a LARGE number of partitions for SOME situations can be valuable.


- The architecture should be easy to understand and comprehend.  This means that
  optimizations in the framework can be easily implemented for new and external
  developers.


- next

    - Create a mapper thread for EACH input...
    -     





- On a 1TB disk, how fast do we want it to be available again over gigabit
  ethernet...
    

- step1...

    - benchmark the performance of taking a graph adjacency list / sparse matrix
      in the form of source, targets... and then invert it into target:
      sources... so that I can count the number of sources.

        the first map job should emit:
            (T1, S1)
            (T2, S1)
            (T3, S1)
            (T1, S2)

      And then I should group them like:

        (T1, (S1,S2))
        (T2, (S1))
        (T3, (S1))

      Then the reduce function should return:

        T1, 2
        T2, 1
        T3, 1


- partition setup... start with the number of machines.  The algorithm for
  determining the number of partitions is:

    nr_partitions = nr_hosts


- Using LARGER numbers of partitions means that when a machine fails that I can
  restore to TWO machines and NOT just one...   TWO machines as the source and
  TWO as the target.  CLEARLY there is a sweet spot.


    - 1000 paritions + 100MB buffer == 100k per chunk written to disk.

    - potentially store all files to disk. how long would it take to read 100k
      at 100MB/s 1ms ... so this is 50% or the original performance... 



        

- One reason that the Pregel API is really cool is that you could implement the
  SAME framework on a single machine instance without a massive amount of wasted
  CPU time due to map reduce.  It COULD even be parallelized somewhat... 
        
- If the config is :

    - partition0: host0, host1, host2
    - partition1: host0, host1, host2

- The route() partition logic should be done via dependency injection so that we
  can make it faster by not having to do an isHashcode comparison for each
  route() usage.  This is a tight loop
    

- how do we store input text so that cat() works efficiently... with a custom
  partition handler.  Make it so tat lines 1-1000 show up on partition 0,
  1001-2000 on parittion 2, etc.
    

- AHHH!!! I put THREE partitions on EACH node... then I can run one reduce on
  EACH for each partition.  Since these boxes replicate these blocks, I can run
  ONE reduce per partition across all three hosts for VERY even CPU
  utilization.  If one of the boxes fails, the controller runs the job on one of
  the backup hosts.
    


- I need to think more about how I want to handle the shuffle stage... I think
  the Mapper should keep a handle on communications channels to shards and that
  ShuffleManager should JUST keep track of chunk job output ... this way if
  they're sorted it can just do a merge joing of the output on disk.

        
        
- we should fallocate chunks , especially chunks that are being written to by
  reducers, so they are on disk contibuous.
        

- I could compute my own minimum spanning tree so that I can route chunks from
  the controller directly and have machines distribute the data FOR me... The
  ideal design will be complicated and involve reading a few more research
  papers on the subject.
  
- compute hashcodes when reading and writing files... then compare the hashcode
  at the end to verify that I have the correct output.
        
- ExtractMap

- ReduceMap, MergeMap

- ReduceMerge (when one of the merge inputs comes from the reduce phase)
    - pagerank would require this...

- ReduceMergeMap ... 


- ReduceLoad
- MergeLoad
- instead of storing the output locally, just load it back into the remote
  storage system.

- The reduce phase should basically be a tiled mere join...

    - each chunk source in the reduce phase is a tile.

    - if the chunk does not have entries that are sorted, first sort each one, 

    - if we do NOT need to

    - the map output might in theory MUCH larger than the map input.
    
        
- Allow the programmer to work with bytes directly for performance reasons.
        
- Make the system so that the Extract phase can actually be the begging of the
  first map/reduce phase. Instead of FIRST writing to the DFS and THEN importing
  the data, if it is already ready to be mapped just read it and stream into the
  map system.

  
        
- The system is modeled after ETL jobs in the form of Extract, Transform, Load:

    http://en.wikipedia.org/wiki/Extract,_transform,_load

    Data is first written into maprunner by using an ExtractWriter and sending
    data to a given path in the system.  Input data is in the form of (K,V) and
    routed to the correct shard based on hash(K).

- The system was designed to be INSANELY small and easy to understand.  The
  original Map Reduce paper makes a great argument that all applications using
  the framework willr realize any performance optimization in the core.

  However, if your core is 100-200k lines of code, this is difficult to
  optimize.
  
- Data should naturally partition without much skew based on the same hash
  function between iterations.

   - Data based on fetching URLs would be a good example.

- Optimized for short running ETL jobs.  Data is designed to be stored within
  exported within maprunner on the local DFS points but isn't meant to be used
  to serve operational reads.  At the end of your map jobs you should export the
  data to your cluster in a system like Cassandra, MySQL, HBase, etc.

- The major optimization is for tasks that join against previously executed map
  reduce jobs.  This optimization is present because we shard the data and store
  the data in a given key on the same machine.  This way it's possible to do a
  map-side join of the data reading from the local host.

 - 
