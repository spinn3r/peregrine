
- Key and Value should be StructReaders / StructWriters
    - What does Hadoop do by default right now ? It uses IntReader IntWriter correct?

        map( StructReader key, StructReader value );
        reduce( StructReader key , List<StructReader> values );

- move to the RangePartitionRouter instead of the HashPartitionRouter

- checksums (I think this part would be easy).

- documentation:
  - citations (pregel, flume, etc)
  - paper
        
- pipeline support and an API or handling them... 

- we need a combiner.

- Fixed number threads

- ChannelWriter and BufferedChannelWriter so that we can elide writes and send
  them in 16k chunks.

- compression

- No MultiOutputStream but instead ALWAYS use pipeline writes

- Job killing so that the controller can kill off something that is running
  easily ... 

- audit ALL the code to look for any problems.

- ChunkMerger, ChunkSorter, and LocalMerger all use a nearly identical priority
  queue merging system and this will need to be cleaned up to reduce the amount
  of code (and potential bugs)

- (done) No use of AsyncOutputStream any more... DefaultChunkWriter still uses
  it... It *could* change to AsyncChannelWriter in the future though.

- Audit everything to make sure we're using zero copy as much as possible.

- O_NOATIME on files ... ?

- During intermediate merges we de-serialze the entire record then add the
  new records when in reality we can just append bytes together ... 

- Do a pass to find out what code we can remove.

- run some profiles on the code to make sure we're performing reasonably.

- review ALL my notes.
