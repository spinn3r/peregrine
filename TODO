

- we need support for fallocate and fadvise (test them on startup).

- ok.. so the current sort algorithm will NO LONGER work on these large jobs due
  to the high number of intermediate files.  I will have to implement a
  multi-pass merge system now... which will take me some time but isn't too
  hard.


- before a job is executed we need to make sure we aren't running something
  already on the cluster... this is a GOOD reason why we should use a secondary
  JVM for jobs so we can just kill it 

- Refactor DefaultChunkWriter to use BufferedChannelBuffer ... 

- ExtractWriter needs buffered IO ... the throughput was nearly 2x ... I think
  the ShuffleSender needs the same change.

- The ExtractWriter should probably write to a buffer BEFORE writing to the
  queue that is used in the HttpClient so that all writes are about 16k ... This
  is what DefaultChunkWriter does and this was done for performance reasons.

- Init of ExtractWriter / DefaultPartitionWriter needs to be fork/join HTTP ... 

- tests take FAR too much time... the more host we add the SLOWER things get due
  to overhead but I think we can/should fix that problem.

- LocalPartitionReader should FAIL when trying to read from a partition which is
  not local according to the config.

- We're still creating too many threads because of newCachedThreadPool and it's
  causing the server to run our of file handles...


  
- Speculative execution is currently disabled because we weren't handling write
  pipelineing and were having filesystem corruption. 

  
- The complete and failed need to keep track of which jobs they're running on
  ... if we report that we failed or completed a job that is ALREADY complete we
  need to ignore this message.

  

- I need to work on the ChunkMerger system so that it can use more memory NOT
  just the size of chunk files. 


- I don't like the reduce sorting framework class names... they are too retro:
    ChunkSorter
    ChunkMerger
    SortResult
    Toplevelsortentryfactory ... 
  


    
- If the CPU performance of performing a sort is MUCH slower than the disk
  perforamnce, we can go ahead and replicate the sort temporary files so that
  hte other replicas can resume if necessary.

      


  
- Make sure the controller and the slaves ALL have the same partitioning
  config...  I imagine this could JUST be a hash of the config and or
  partitioning setup.



- ChunkMerger, ChunkSorter, and LocalMerger all use a nearly identical priority
  queue merging system and this will need to be cleaned up to reduce the amount
  of code (and potential bugs)

- our async output stream support is lame because we should REALLY be using
  direct DMA zerocopy directly to the disk.

- stability bugs:

    - we need to use ChannelBuffers everywhere
    - some of the thread local bugs are problematic with StructWriter
    
- Potentially support fallocate so that I can allocate files in extents of say
  10MB.
    
- on startup verify that we have enough memory... this should be done in the
  controller and the FS daemon. 

- test code which tests for off by one errors by looking at the size of buffers
  and changing their inputs to see if rolling over breaks.

- migrate AsyncOutputStream to use mmap and then force() so that I can write
  ChannelBuffers to it.. by calling toByteBuffer on it... 

- O_NOATIME on files ... ?

- to pin pages into memory what I should probably do is mmap the file
  TWICE... once with MLOCK and then with the normal java APIs and then close
  them both.

- We should support fadvising away files once we've processed them to avoid VFS
  page cache insanity.  ESPECIALLY some of the temporary sort files.  

- suppprt fixed width input formats.  There are a LOT of advantages to doing
  this:

    - I don't have to read varints when parsing the key list
    - I can store the key list and value list side by side.

    - when sorting, within the intermediate files, I can JUST write a new key
      list and then keep the values in the same spot on disk.


- right now we fail to handle disks filling up..  We need test for all file
  writes.

- some items in StructReader and StructWriter need to be lenght prefixed.  For
  example byte[] and String.  However, this is redundand if they're the only
  item in the set.  Perhaps a fixed width version which doesn't require length prefixed.

    writeByteArray() writeByteArrayFixed() 
    readByteArray() readByteArrayFixed();

- I need to FULLY solve the issue of endianness in peregrine because ByteBuffer
  and netty will force me to solve it.

- Map and Reduce BOTH need to support merge() operations... We ALSO need to
  support APPEND to files ... so that a new Map or Reduce job can just write to
  the new file.

    - We can make pagerank work WITHOUT this thoug... MapperTask and MergerTask
      could in theory (now) be merged...

- The way we're handling broadcast values will yield TOO MANY values for chunks
  ... should I expose the listener interface for map jobs?

- Consider an implementation of this sorting algorithm that uses an in-place
  sort on disk with sequential IO which would allow us to use less disk space...

    - I don't really need to because once I'm done with chunks, I can write a
      progress file, delete the previous chunk file, then continue,  This would
      require CHUNK_SIZE additional bytes which isn't the end of the world.
  
- write a few tests of sequential pseudo sequential read and write IO
  performance.  This involved running two writers to two files on a single HDD
  ...

  Test these scenarios:

    - As writes come in , write them to individual files directly.
        - also test with fallocate() or say 500k-5MB extents
        
    - Buffered writes so that we lay them down as individual files but then
      reads have to jump over regions within the file.  This would yield a
      sequential file BUT I would have to seek over large portions of it...

        - 1000 partitions... writing to us in chunks

