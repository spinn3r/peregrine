


StructWriter refactor notes:


    - remove .keys and .values packages

    - Allocating a ThreadLocal value in the new framework is NOT a good idea I
      think because I can create TWO StructWriters and I need to use a NEW
      buffer for the second one.  The problem is what should be the default
      size?  Probably 8 bytes... to store one primitive.

        - I *could* pool these by using direct channel buffers.

    - HashSetValue seems the SAME as Struct I think.
        - refactor this AFTER I get everything working probably..

        - they're basically the same but one is fixed width and the other is
          variable.

    - The DefaultChunkWriter I think is broken... try to test it with just raw
      byte arrays.

      - 






- Remove AsyncOutputStream

    - Rename MultiOutputStream to MultiChannelBufferWritable

    - Move ChannelBufferWritable to peregrine.util.netty



- If I ONLY use the OS buffer I can avoid issues with having to pool buffers.

- https://issues.apache.org/jira/browse/HADOOP-6148

    - use the native zlib with largish buffers for computing the checksum.


- if we do speculative execution, I need a way to say "stop doing work" to a
  node...  I guess I could check BETWEEN files as I process them in the
  map/merge task and I would need similar checks in the reduce phase. 

- we need support for fallocate and fadvise (test them on startup).

- I need to implement break points so that a job can terminate itself.

    - in between map / merge chunks and between reduce files as they are written
      to disk.


- before a job is executed we need to make sure we aren't running something
  already on the cluster... this is a GOOD reason why we should use a secondary
  JVM for jobs so we can just kill it 

- Refactor DefaultChunkWriter to use BufferedChannelBuffer ... 

- ExtractWriter needs buffered IO ... the throughput was nearly 2x ... I think
  the ShuffleSender needs the same change.

- The ExtractWriter should probably write to a buffer BEFORE writing to the
  queue that is used in the HttpClient so that all writes are about 16k ... This
  is what DefaultChunkWriter does and this was done for performance reasons.

- Init of ExtractWriter / DefaultPartitionWriter needs to be fork/join HTTP ... 

- tests take FAR too much time... the more host we add the SLOWER things get due
  to overhead but I think we can/should fix that problem.

- LocalPartitionReader should FAIL when trying to read from a partition which is
  not local according to the config.

- We're still creating too many threads because of newCachedThreadPool and it's
  causing the server to run our of file handles...


  
- Speculative execution is currently disabled because we weren't handling write
  pipelineing and were having filesystem corruption. 

  
- The complete and failed need to keep track of which jobs they're running on
  ... if we report that we failed or completed a job that is ALREADY complete we
  need to ignore this message.

  

- I need to work on the ChunkMerger system so that it can use more memory NOT
  just the size of chunk files. 


- I don't like the reduce sorting framework class names... they are too retro:
    ChunkSorter
    ChunkMerger
    SortResult
    Toplevelsortentryfactory ... 
  


    
- If the CPU performance of performing a sort is MUCH slower than the disk
  perforamnce, we can go ahead and replicate the sort temporary files so that
  hte other replicas can resume if necessary.

      


  
- Make sure the controller and the slaves ALL have the same partitioning
  config...  I imagine this could JUST be a hash of the config and or
  partitioning setup.



- ChunkMerger, ChunkSorter, and LocalMerger all use a nearly identical priority
  queue merging system and this will need to be cleaned up to reduce the amount
  of code (and potential bugs)

- our async output stream support is lame because we should REALLY be using
  direct DMA zerocopy directly to the disk.

- stability bugs:

    - we need to use ChannelBuffers everywhere
    - some of the thread local bugs are problematic with StructWriter
    
    

- test code which tests for off by one errors by looking at the size of buffers
  and changing their inputs to see if rolling over breaks.

- migrate AsyncOutputStream to use mmap and then force() so that I can write
  ChannelBuffers to it.. by calling toByteBuffer on it... 

- O_NOATIME on files ... ?


- We should support fadvising away files once we've processed them to avoid VFS
  page cache insanity.  ESPECIALLY some of the temporary sort files.  

- suppprt fixed width input formats.  There are a LOT of advantages to doing
  this:

    - I don't have to read varints when parsing the key list
    - I can store the key list and value list side by side.

    - when sorting, within the intermediate files, I can JUST write a new key
      list and then keep the values in the same spot on disk.


- right now we fail to handle disks filling up..  We need test for all file
  writes.

- some items in StructReader and StructWriter need to be lenght prefixed.  For
  example byte[] and String.  However, this is redundand if they're the only
  item in the set.  Perhaps a fixed width version which doesn't require length prefixed.

    writeByteArray() writeByteArrayFixed() 
    readByteArray() readByteArrayFixed();

- I need to FULLY solve the issue of endianness in peregrine because ByteBuffer
  and netty will force me to solve it.


- The way we're handling broadcast values will yield TOO MANY values for chunks
  ... should I expose the listener interface for map jobs?

