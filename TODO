

- Map and Reduce BOTH need to support merge() operations... We ALSO need to
  support APPEND to files ... so that a new Map or Reduce job can just write to
  the new file.

    - We can make pagerank work WITHOUT this thoug... MapperTask and MergerTask
      could in theory (now) be merged...


- The shuffle phase needs to support async IO but then write everything to a
  BlockingQueue and this queue is then drained by a single thread.

    I think file PUTs could do the same thing by having a single thread doing
    the writing to a file... The SAME system could be used for reading from
    files.



- When writing a new file, we need to require that it is removed first or throw
  an Exception.  But for now we need a way to do this ... 

- Filesystem operations like touch() should actually be done with reduce() or
  map only jobs that emit no output... I think. 



- The way we're handling broadcast values will yield TOO MANY values for chunks
  ... should I expose the listener interface for map jobs?



- I think that Joiner should be called Merger similar to Mapper and Reducer
  ... in fact this is probably the right thing to do because even though the
  logistics are 'join' I'd rather it be modeled after the Map , Reduce , Merge
  paper.

- IntValue should be efficient and maybe even use a VarInt ... right now it
  always uses 4 bytes which is hella lame.

- MapOutputSortCallable should become MapOutputReducer or something ... not
  EVERYTHING needs to be called *Callable.


- What happens if a priority queue has TWO of the same item in it?




- Change the ChunkReader interface to use:

    while ( reader.hasNext() ) {
        key = reader.getKey();
        value = reader.getValue();
    }

- Consider an implementation of this sorting algorithm that uses an in-place
  sort on disk with sequential IO which would allow us to use less disk space...

    - I don't really need to because once I'm done with chunks, I can write a
      progress file, delete the previous chunk file, then continue,  This would
      require CHUNK_SIZE additional bytes which isn't the end of the world.
  
- write a few tests of sequential pseudo sequential read and write IO
  performance.  This involved running two writers to two files on a single HDD
  ...

  Test these scenarios:

    - As writes come in , write them to individual files directly.
        - also test with fallocate() or say 500k-5MB extents
        
    - Buffered writes so that we lay them down as individual files but then
      reads have to jump over regions within the file.  This would yield a
      sequential file BUT I would have to seek over large portions of it...

        - 1000 partitions... writing to us in chunks


- support for doing a 'cat' of a file and factoring in the toString() methods of
  the underlying type... this will be used for debug purposes.

    










- It COULD be that MapOutputIndex is being corrupted... try to make THAT
  synchronous ... 

- TODO:

    - MapOutputBuffer needs to be synchronized becuase for SOME reason it ends
      up corupted.  This defeats the whole point of being parallel.

    - Unit test to run full map and reduce phase... 


- FIXME: I need to now create some unit tests that verify the correct design:

    - writes to muliple chunk files
    - make sure chunk files get schedule for execution on the correct CPU/thread
    - verify that algorithm execution is correct
        
- The current sort doesn't place things under the same key which is wrong.


- What is the timeframe for checksums?  SHA1 ? MD5 ?
