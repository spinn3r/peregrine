


Additional burton-struct-api-change before merging.


    - remove .keys and .values packages

    - Allocating a ThreadLocal value in the new framework is NOT a good idea I
      think because I can create TWO StructWriters and I need to use a NEW
      buffer for the second one.  The problem is what should be the default
      size?  Probably 8 bytes... to store one primitive.

        - I *could* pool these by using direct channel buffers.

    - HashSetValue seems the SAME as Struct I think.
        - refactor this AFTER I get everything working probably..

        - they're basically the same but one is fixed width and the other is
          variable.

        - 
          
    - The DefaultChunkWriter I think is broken... try to test it with just raw
      byte arrays.

    - which tests fail so I can write reductions for them and fix them?

        - these tests fail...

        TEST-peregrine.TestBroadcastMapReduce.txt
        TEST-peregrine.TestMapReduce.txt
        TEST-peregrine.TestMapReduceWithMergeFactor.txt
        TEST-peregrine.TestPagerank.txt
        TEST-peregrine.io.TestFullOuterJoin.txt
        TEST-peregrine.shuffle.TestParallelShuffleInputChunkReader.txt
        TEST-peregrine.shuffle.TestShuffleInputChunkReader.txt
        


    - the issue is that pulling apart the packets isn't working... 
        - what components could be failing 
        - sending the packets to the server
        - receiving the packets and writing them to the shuffle output writer
        - writing them to disk
        - reading them from disk
        

        - come up with a BASIC assertion test...    
          - write ONE key/value with unique setup to one partition on one server... then
            read the values back out and make sure they look ok.



- centralizing log4j initialization needs to be done.

- If I were to use an array backed queue which was just long[] I could then
  allocate from a direct memory ring buffer and read pointers and lengths from
  them for sending data over the wire.


- Split reading and writing in the file mapper into two separate classes.


- Another architecture I could have had could have been ONE primary listening
  daemon which then forks one mapper/reducer proc PER run.  One downside is that
  for the shuffling I'd have to do soem shared memory work to or fifos to
  communicate between them for shuffling.



- If I ONLY use the OS buffer I can avoid issues with having to pool buffers.

- CRC32 support https://issues.apache.org/jira/browse/HADOOP-6148

    - Use the native zlib with largish buffers for computing the checksum.

    - should there be a generic CRC32 container because we ALSO need this for
      shuffling.
        

- If we do speculative execution, I need a way to say "stop doing work" to a
  node...  I guess I could check BETWEEN files as I process them in the
  map/merge task and I would need similar checks in the reduce phase. 

  
- I need to implement break points so that a job can terminate itself.

    - In between map / merge chunks and between reduce files as they are written
      to disk...

        - Another way to do this is to terminate the 


- ExtractWriter needs buffered IO ... the throughput was nearly 2x ... I think
  the ShuffleSender needs the same change.

- The ExtractWriter should probably write to a buffer BEFORE writing to the
  queue that is used in the HttpClient so that all writes are about 16k ... This
  is what DefaultChunkWriter does and this was done for performance reasons.

- Init of ExtractWriter / DefaultPartitionWriter needs to be fork/join HTTP ... 

- tests take FAR too much time... the more host we add the SLOWER things get due
  to overhead but I think we can/should fix that problem.

- LocalPartitionReader should FAIL when trying to read from a partition which is
  not local according to the config.

- We're still creating too many threads because of newCachedThreadPool and it's
  causing the server to run our of file handles...


  
- Speculative execution is currently disabled because we weren't handling write
  pipelineing and were having filesystem corruption. 

  
- The complete and failed need to keep track of which jobs they're running on
  ... if we report that we failed or completed a job that is ALREADY complete we
  need to ignore this message.

  

- I need to work on the ChunkMerger system so that it can use more memory NOT
  just the size of chunk files. 


- I don't like the reduce sorting framework class names... they are too retro:
    ChunkSorter
    ChunkMerger
    SortResult
    Toplevelsortentryfactory ... 
  


    
- If the CPU performance of performing a sort is MUCH slower than the disk
  perforamnce, we can go ahead and replicate the sort temporary files so that
  hte other replicas can resume if necessary.

      


  
- Make sure the controller and the slaves ALL have the same partitioning
  config...  I imagine this could JUST be a hash of the config and or
  partitioning setup.



- ChunkMerger, ChunkSorter, and LocalMerger all use a nearly identical priority
  queue merging system and this will need to be cleaned up to reduce the amount
  of code (and potential bugs)

- our async output stream support is lame because we should REALLY be using
  direct DMA zerocopy directly to the disk.

- stability bugs:

    - we need to use ChannelBuffers everywhere
    - some of the thread local bugs are problematic with StructWriter
    
    

- test code which tests for off by one errors by looking at the size of buffers
  and changing their inputs to see if rolling over breaks.

- migrate AsyncOutputStream to use mmap and then force() so that I can write
  ChannelBuffers to it.. by calling toByteBuffer on it... 

- O_NOATIME on files ... ?


- We should support fadvising away files once we've processed them to avoid VFS
  page cache insanity.  ESPECIALLY some of the temporary sort files.  

- suppprt fixed width input formats.  There are a LOT of advantages to doing
  this:

    - I don't have to read varints when parsing the key list
    - I can store the key list and value list side by side.

    - when sorting, within the intermediate files, I can JUST write a new key
      list and then keep the values in the same spot on disk.


- right now we fail to handle disks filling up..  We need test for all file
  writes.

- some items in StructReader and StructWriter need to be lenght prefixed.  For
  example byte[] and String.  However, this is redundand if they're the only
  item in the set.  Perhaps a fixed width version which doesn't require length prefixed.

    writeByteArray() writeByteArrayFixed() 
    readByteArray() readByteArrayFixed();

- I need to FULLY solve the issue of endianness in peregrine because ByteBuffer
  and netty will force me to solve it.


- The way we're handling broadcast values will yield TOO MANY values for chunks
  ... should I expose the listener interface for map jobs?

