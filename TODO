
- we're at 100% CPU in the extract / init phase of our pagerank tests.

  http://docs.jboss.org/netty/3.2/api/org/jboss/netty/channel/socket/SocketChannelConfig.html

    - log those on startup

- I need to work on the ChunkMerger system so that it can use more memory NOT
  just the size of chunk files. 


- ChunkMerger, ChunkSorter, and LocalMerger all use a nearly identical priority
  queue merging system and this will need to be cleaned up... 


- our async output stream support is lame because we should REALLY be using
  direct DMA zerocopy right to the disk.

- stability bugs:

    - we need to use ChannelBuffers everywhere
    - some of the thread local bugs are problematic with StructWriter
    - 

- potentially support fallocate so that I can clean up after ... 
    
- on startup verify that we have enough memory... this should be done in the
  controller and the FS daemon. 

- code which tests for off by one errors by looking at the size of buffers and
  changing their inputs to see if rolling over breaks.

- migrate AsyncOutputStream to use mmap and then force() so that I can write
  ChannelBuffers to it.. by calling toByteBuffer on it... 

- O_NOATIME on files ... ?

- to pin pages into memory what I should probably do is mmap the file
  TWICE... once with MLOCK and then with the normal java APIs and then close
  them both.

- We should support fadvising away files once we've processed them to avoid VFS
  page cache insanity.  ESPECIALLY some of the temporary sort files.  

- suppprt fixed width input formats.  There are a LOT of advantages to doing
  this:

    - I don't have to read varints when parsing the key list
    - I can store the key list and value list side by side.

    - when sorting, within the intermediate files, I can JUST write a new key
      list and then keep the values in the same spot on disk.

- Write MEMORY benchmarks... that measure heap BEFORE and AFTER each test so that
  I can prevent memory leaks.

  - Constrain the JVM memory that is forked from <junit> so that I can verify
    that the memory NEVER goes above a certain lower bound.

- If the ExtractWriter does NOT write to all hosts the HTTP server is still
  written to and then creates a fucked up chunk.  One way to test this is to
  write ONE key so that not every host has a value and peregrine will then die.

- The PUT handler should ONLY return HTTP 200 OK once the file is WRITEN and
  should NOT leave a temp file in its place...

- fails to handle disks filling up...... 

- log output should be written to /var/log/peregrine-port/

- some items in StructReader and StructWriter need to be lenght prefixed.  For
  example byte[] and String.  However, this is redundand if they're the only
  item in the set.  Perhaps a fixed width version which doesn't require length prefixed.

    writeByteArray() writeByteArrayFixed() 
    readByteArray() readByteArrayFixed();

- I need to FULLY solve the issue of endianness in peregrine because ByteBuffer
  and netty will force me to solve it.

- Map and Reduce BOTH need to support merge() operations... We ALSO need to
  support APPEND to files ... so that a new Map or Reduce job can just write to
  the new file.

    - We can make pagerank work WITHOUT this thoug... MapperTask and MergerTask
      could in theory (now) be merged...

- The way we're handling broadcast values will yield TOO MANY values for chunks
  ... should I expose the listener interface for map jobs?

- Consider an implementation of this sorting algorithm that uses an in-place
  sort on disk with sequential IO which would allow us to use less disk space...

    - I don't really need to because once I'm done with chunks, I can write a
      progress file, delete the previous chunk file, then continue,  This would
      require CHUNK_SIZE additional bytes which isn't the end of the world.
  
- write a few tests of sequential pseudo sequential read and write IO
  performance.  This involved running two writers to two files on a single HDD
  ...

  Test these scenarios:

    - As writes come in , write them to individual files directly.
        - also test with fallocate() or say 500k-5MB extents
        
    - Buffered writes so that we lay them down as individual files but then
      reads have to jump over regions within the file.  This would yield a
      sequential file BUT I would have to seek over large portions of it...

        - 1000 partitions... writing to us in chunks

- support for doing a 'cat' of a file and factoring in the toString() methods of
  the underlying type... this will be used for debug purposes.

- It COULD be that MapOutputIndex is being corrupted... try to make THAT
  synchronous ... 
