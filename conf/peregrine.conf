
###
#
# The controller which will send and receive job RPC messages.  This should be
# the machine you execute your jobs from and the controller should be up and
# online for the full completion of the job.
#
controller=dev3.wdc.sl.spinn3r.com:11112

###
#
# Control port for normal node operations (PFS, job scheduling, etc).  This
# applies to both the controller and compute nodes.
#
port=11112

###
#
# PFS root directory for filesystem operation.
#
root=/d2/peregrine-fs

###
#
# The number of partitions to deploy per host.  10-25 is a safe number.  The
# more partitions you have the faster recovery will be but the greater impact
# you will have in performance.
#
partitions_per_host=8

###
# 
# Host concurrency in order to determine how many jobs can run on hosts at a
# given time.  Note that if `partitions_per_host` is less than `concurrency`
# then the lower bound applies.  AKA your effective concurrency is:
#
#  min( partitions_per_host, concurrency )
#
# Memory requirements:
#
# The memory requirements are:
# 
#  (2 x chunk_size * concurency) + (2 x shuffle_size).  
#
# Normally:
#   chunk_size     = 128MB
#   shuffle_size   = 128MB
#
# so with concurency = 8 this would require 2.4GB of memory per node.
#
# You should also allow for about 256MB of memory for JVM overhead.

#concurrency=1
concurrency=8

###
#
# NR replicas to maintain per partition.
# 
# The more hosts you have in your cluster the better as this will allow greater
# parallel recovery.  The maximum parallel recovery can be provided until the
# number of failed hosts is > nr_hosts - nr_replicas at which point we won't be
# able to evenly distribute partitions among new hosts.
#
replicas=1
